<!DOCTYPE html>
<style>
body {
    background-color: #28282d;
    color: #a0a0a0;
    font-family: system-ui,sans-serif;
    font-size: 1em;
}
h1, h2 {
    color: #cecece;
}
h4 {
    text-decoration: underline;
}
footer {
    font-family: monospace;
}
a:link {
    color: #4182cd;
    background-color: transparent;
    text-decoration: none;
}
a:visited {
    color: #c83c64;
    background-color: transparent;
    text-decoration: none;
}
a:hover {
    color: #cecece;
    background-color: transparent;
    text-decoration: underline;
}
a:active {
    color: #cecece;
    background-color: transparent;
    text-decoration: underline;
}
</style>
<html>
    <header>
        <title>Vivien Cabannes</title>
    </header>
    <body>
        <h1>
            Vivien Cabannes
        </h1>
        <p>
            My current research interests revolve around mechanistic interpretability and large-scale experimental design.<br>
            Other former interests include representation learning (notably with self-supervised learning), active learning, statistical learning theory, and weakly supervised learning.<br>
            Hobbies include art with AI and broader reflections on technology and society.
        </p>
        <h2>
            Publications
        </h2>
        <h4>
            Deep Learning Understanding
        </h4>
        <p>
            Vivien Cabannes, Berfin &#350;im&#351;ek, and Alberto Bietti,
	    <a href="https://arxiv.org/abs/2402.18724">Learning Associative Memories with Gradient Descent.</a>
            <em>Preprint</em>,
            2024.<br>

            Vivien Cabannes, Elvis Dohmatob, and Alberto Bietti,
            <a href="https://arxiv.org/abs/2310.02984">Scaling laws for associative memory.</a>
            In <em>ICLR</em> (Spotlight),
            2024.<br>

            Jenny Bao, Aaron Defazio, Alberto Bietti, and Vivien Cabannes,
            Hessian inertia.
            In <em>ICML Learning Dynamics Workshop</em>,
            2023.<br>

            Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herv&#233; Jegou, and L&#233;on Bottou,
            <a href="https://arxiv.org/abs/2306.00802">Birth of a transformer: a memory viewpoint.</a>
            In <em>NeurIPS</em> (Spotlight),
            2023.
        </p>
        <h4>
            Representation Learning & Self-Supervised Learning
        </h4>
        <p>
            Vivien Cabannes and Francis Bach,
            <a href="https://arxiv.org/abs/2306.00742">The Galerkin method beats Graph-Based Approaches for Spectral Algorithms.</a>
            In <em>AISTATS</em>,
            2024.<br>

            Vivien Cabannes, Bobak Kiani, Randall Balestriero, Yann LeCun, and Alberto Bietti,
            <a href="https://arxiv.org/abs/2302.02774">The SSL interplay: augmentations, inductive bias, and generalization.</a>
            In <em>ICML</em>,
            2023.<br>

            Vivien Cabannes, Alberto Bietti, and Randall Balestriero,
            <a href="https://arxiv.org/abs/2211.03782">On minimal variations for unsupervised representation learning.</a>
            In <em>ICASSP</em> (Oral),
            2023.
        </p>
        <h4>
            Active Learning
        </h4>
        <p>
            Charles Arnal*, Vivien Cabannes*, and Vianney Perchet,
            <a href="https://arxiv.org/abs/2402.13079">Mode estimation with partial feedback.</a>
            <em>Preprint</em>,
            2024.<br>

            Vivien Cabannes, L&#233;on Bottou, Yann LeCun and Randall Balestriero,
            <a href="https://arxiv.org/abs/2303.15256">Active self-supervised Learning: a few low-cost relationships are all you need.</a>
            In <em>ICCV</em>,
            2023.<br>

            Vivien Cabannes, Vianney Perchet, Francis Bach, and Alessandro Rudi,
            <a href="https://arxiv.org/abs/2205.13255">Active labeling: streaming stochastic gradients.</a>
            In <em>NeurIPS</em>,
            2022.
        </p>
        <h4>
            Learning Theory
        </h4>
        <p>
            Vivien Cabannes,
            <a href="https://arxiv.org/abs/2306.11928">Open problem: learning with variational objectives on measures.</a>
            In <em>IEEE Big Data</em>,
            2023.<br>

            Vivien Cabannes and Stefano Vigogna,
            <a href="https://arxiv.org/abs/2305.16014">How many samples are needed to leverage smoothness?</a>
            In <em>NeurIPS</em>,
            2023.<br>

            Vivien Cabannes and Stefano Vigogna,
            <a href="https://arxiv.org/abs/2205.10055">A case of exponential convergence rates for SVM.</a>
            In <em>AISTATS</em>,
            2023.<br>

            Vivien Cabannes, Alessandro Rudi, and Francis Bach,
            <a href="https://arxiv.org/abs/2102.00760">Fast rates in structured prediction.</a>
            In <em>COLT</em>,
            2021.
        </p>
        <h4>
            Weakly Supervised Learning
        </h4>
        <p>
            Vivien Cabannes,
            <a href="https://arxiv.org/abs/2209.11629">From weakly supervised learning to active labeling.</a>
            <em>PhD thesis at Ecole Normale Sup&#233;rieure</em>,
            2022.<br>

            Vivien Cabannes, Loucas Pillaud-Vivien, Francis Bach, and Alessandro Rudi,
            <a href="https://arxiv.org/abs/2009.04324">Overcoming the curse of dimensionnality with Laplacian regularization in semi-supervised learning.</a>
            In <em>NeurIPS</em>,
            2021.<br>

            Vivien Cabannes, Francis Bach, and Alessandro Rudi,
            <a href="https://arxiv.org/abs/2102.02789">Disambiguation of weak supervision with exponential convergence rates.</a>
            In <em>ICML</em>,
            2021.<br>

            Vivien Cabannes, Alessandro Rudi, and Francis Bach,
            <a href="https://arxiv.org/abs/2003.00920">Structured prediction with partial labelling through the infimum loss.</a>
            In <em>ICML</em>,
            2020.
        </p>
        <h4>
            Sampling
        </h4>
        <p>
            Vivien Cabannes, and Charles Arnal,
            <a href="https://arxiv.org/abs/2311.13845">Touring sampling with pushforward maps.</a>
            In <em>ICASSP</em> (Best Paper Award for Industry),
            2024.<br>
        </p>
        <h4>
            Art with AI
        </h4>
        <p>
            Vivien Cabannes*, Thomas Kerdreux*, and Louis Thiry,
            <a href="https://arxiv.org/abs/2010.13864">Diptychs of human and machine perceptions.</a>
            In <em>NeurIPS Creativity Workshop</em>,
            2020.<br>

            Vivien Cabannes*, Thomas Kerdreux*, Louis Thiry*, and Tina & Charly*,
            <a href="https://arxiv.org/abs/1910.04386">Dialog on a canvas with a machine.</a>
            In <em>NeurIPS Creativity Workshop</em>,
            2019.
        </p>
        <h4>
            Technology and Society
        </h4>
        <p>
            Vivien Cabannes,
            <a href="https://www.cairn-int.info/journal-esprit-2022-7-page-117.htm">Will the digital future be embodied?</a>
            In <em>Revue Esprit</em>,
            2022.
        </p>

        *Equal contributions

        <h2>
            Curriculum Vitae
        </h2>
        <p>
            2022-2024: Postdoctoral researcher at Meta, supervised by L&#233;on Bottou<br>
            2019-2022: PhD in Machine Learning, supervised by Francis Bach and Alessandro Rudi<br>
            2018: Year as a quant at Point72, supervised by Cyril Deremble<br>
            2017: Master in machine learning (MVA)<br>
            2014: Entrance at the ENS in math<br>
            2012: Baccalaur&#233;at<br>
            <a href="files/resume.pdf">PDF version</a>
        </p>
    </body>
    <footer>
        <p>
            --- Updated on March 26th, 2024 ---
        </p>
    </footer>
</html>
